{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Make sure you change the start date to today so we can ensure there is data for today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Import NYC yellow cab data from Azure Open Datasets\r\n",
        "from azureml.opendatasets import NycTlcYellow\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "end_date = parser.parse('2018-07-08 23:59:59')\r\n",
        "start_date = parser.parse('2018-01-01 00:00:00')\r\n",
        "\r\n",
        "nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
        "nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We're using the NYC Yellow Taxi dataset for our baseline dataset.  This dataset only covers up to 2018 in the public samples, so we need to do some manipulations on the file for our purposes:\r\n",
        "1. We add 4 years to the start and end date to make the data appear current.\r\n",
        "2. We fabricate a column that is just Year/Month/Day for partitioning purposes later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "##  Shift the dates from 2018 -> 2022, and create a field with pickup date\r\n",
        "nyc_tlc_df = nyc_tlc_df.withColumn('tpepPickupDateTime',add_months(col('tpepPickupDateTime'),48))\r\n",
        "nyc_tlc_df = nyc_tlc_df.withColumn('tpepDropoffDateTime',add_months(col('tpepDropoffDateTime'),48))\r\n",
        "nyc_tlc_df = nyc_tlc_df.withColumn('puYear',col('puYear')+4)\r\n",
        "nyc_tlc_df = nyc_tlc_df.withColumn(\"puDate\",to_date(col('tpepPickupDateTime')))\r\n",
        "\r\n",
        "#display(nyc_tlc_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now that we have loaded and manipulated the data set, we need to persist it in our lake.  Here we persist the data partitioned by year, month, and full date and save it as both the parquet and delta formats.\r\n",
        "\r\n",
        "We'll use both of these formats with the Serverless SQL Endpoint in Synapse to illustrate how it affects the performance of the query engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "nyc_tlc_df.write.mode(\"overwrite\").partitionBy(\"puYear\",\"puMonth\",\"puDate\").parquet(\"/output/nycyellow\")\r\n",
        "nyc_tlc_df.write.mode(\"overwrite\").partitionBy(\"puYear\",\"puMonth\",\"puDate\").format(\"delta\").save(\"/output/nycyellowdelta\")\r\n",
        "#nyc_tlc_df.write.mode(\"overwrite\").format(\"delta\").save(\"/output/nycyellowdelta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}